

- 📄 DAY1: Neural Network (NN) Foundation

  - **퍼셉트론(Perceptron)**: 신경망을 이루는 가장 **기본 단위**

    - 초기 형태의 인공 신경망으로 **다수의 입력**으로부터 **하나의 결과**를 내보내는 알고리즘

    - 퍼셉트론 그림

      ![image-20210810154954858](https://user-images.githubusercontent.com/76864400/128843479-db2c8580-0463-4a8e-9dcd-b5ed615efde2.png)

      

      - x: 입력값, W: 가중치, y: 출력값

      - **가중치**: 신경 세포 뉴런에서의 **신호를 전달**하는 축삭돌기의 역할 함

        - 가중치의 값이 크면 클수록 해당 입력 값이 중요 ->  강한 신호 보냄

      - 각 입력값이 가중치와 곱해져 인공 뉴런에 보내지고, 각 입력값과 그에 해당되는 가중치의 곱의 전체 합이 임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 출력 신호로 1을 출력, 아니면 0을 출력 

        ![image-20210810173107878]https://user-images.githubusercontent.com/76864400/128843482-391f9536-aee9-4567-9659-2b8fb1a1fb3c.png

        -> 위 식에서 세타는 임계치(threshold 의미)

      - 위 식에서 임계치를 좌변으로 넘기고 편향(bias)으로 표현 가능

        ![image-20210810173435493](https://user-images.githubusercontent.com/76864400/128843484-90df368b-180c-4ac4-a175-271cc737600d.png)

      - 계단 함수(Step Function)

        ![image-20210810173043531](https://user-images.githubusercontent.com/76864400/128843480-e71b2089-d449-4262-9a4e-191121ce794e.png)

      - 단순한 논리게이트로 본 **단층 퍼셉트론**

        - AND GATE: 두 개의 입력 값이 모두 1인 경우에만 출력값이 1이 나오는 구조
        - NAND GATE: AND GATE 결과의 반대(NOT AND), 두 개의 입력값이 1인 경우에만 출력값이 0이 나오는 구조
        - OR GATE: 두 개의 입력이 모두 0인 경우에 출력값이 0이고 나머지 경우에는 모두 출력값이 1이 나오는 구조
        - 단층 퍼셉트론의 허점
          - 단층 퍼셉트론으로는 XOR 문제(둘이 서로 다를 때만 1을 출력, 배타적 논리합) 해결 불가 -> 하지만 은닉층 늘리면 (다층) 허점 보완 가능

      - **다층 퍼셉트론**

        - 입력층과 출력층 사이에 존재하는 층인 **은닉층**이 존재

        - 다층 퍼셉트론으로 AND, NAND, OR 게이트를 조합하여 XOR 게이트를 구현

          ![image-20210810174728190](https://user-images.githubusercontent.com/76864400/128843456-4b296902-c019-4333-9b5f-8d4df084cfdd.png)

        

  - **인공신경망(Artificial Neural Networks, ANN)**: 인간의 **뉴런 구조**를 본떠 만든 **기계학습 모델**, 인간의 신경세포를 모델링해 기계가 학습하는 것

    - **신경세포(뉴런)**: 신경계를 형성하는 세포로, 전기적인 신호로 서로 통신하며 정보 저장

    - **신경세포(뉴런)** 그림 

      - 뉴런은 가지돌기(dendrites)에서 신호를 받아들이고, 신호가 일정치 이상의 크기를 가지면 축삭돌기(axon hillock) 를 통해 신호 전달
        - Cellbody(신경세포체): 연산을 함

      ![image-20210810154140881](https://user-images.githubusercontent.com/76864400/128843475-ea90a4ff-025d-4754-9d7e-4633e9f9958d.png)

    - **신경망 층**

      - **입력층**(input layer의 노드 개수: feature 수)

        - 데이터셋으로부터 입력 받음
        - 입력 변수의 수 = 입력 노드의 수
        - 입력층은 어떤 계산도 수행하지 않음
        - 신경망의 층수를 셀 때 입력층 포함하지 않음

      - **은닉층**(hidden layer)

        - 계산 일어나는 층이 둘 이상인 신경망은 다층 신경망

        - 계산 없는 입력층과 마지막 출력층 사이에 있는 층들은 은닉층

        - 은닉층에 있는 계산 결과를 사용자가 볼 수 없음

        - 딥러닝은 사실 두 개 이상의 은닉층을 가진 신경망(심층 신경망)

          -> 입력층을 제외하고 세보면 3개 이상의 layer 갖고 있는 신경망 의미함

      - **출력층**(output layer의 노드 개수: 타겟 값, 클래스 수)

        - 활성함수가 존재하는데 활성화함수는 풀고자하는 문제에 따라 다른 종류 사용

        - 회귀문제에서 예측할 목표 변수가 실수값인 경우 활성화함수가 필요 없음 

          → 출력노드의 수는 출력변수의 개수

        - 이진 분류 문제의 경우, 시그모이드 함수를 사용해 출력을 확률값으로 변환해 클래스 결정하도록 함

          →이진분류는 class수가 2개 → (0 아니면 1)

          → 보통 노드 1개 → sigmoid function 사용

        - 다중 클래스를 분류하는 경우 출력층 노드가 분류 수 만큼 존재하며 소프트맥스 함수를 활성화 함수로 사용

      

    - **활성화 함수(Activation Function)**

      - 입력된 데이터의 가중 합을 출력 신호로 변환하는 함수
      - 활성화 함수는 비선형 함수

      ![image-20210810180024355](https://user-images.githubusercontent.com/76864400/128843462-f836df76-15c0-4e57-87b7-7df23a424dd2.png)

      - **계단 함수(step function)**

        ![image-20210810180156993](https://user-images.githubusercontent.com/76864400/128843464-81e3b538-789b-4a28-b5a6-5c7d979e15f8.png)

        - 거의 사용되지 않지만, 퍼셉트론을 통해 처음으로 인공 신경망 배울 때 가장 처음 접하는 활성화 함수

        - 0보다 큰 경우 1 출력, 0보다 작은 경우 0을 출력하는 함수

      - **시그모이드 함수(sigmoid function)**

        ![image-20210810180609931](https://user-images.githubusercontent.com/76864400/128843467-16dff423-8380-485f-9e1d-0728a263aeed.png)

        - 0~1사이 출력하는 함수

      - **렐루 함수(ReLU function)**

        ![image-20210810180819229](https://user-images.githubusercontent.com/76864400/128843471-159bf8fa-6fe3-4402-8394-d53c61e0d5e3.png)

        - 인공 신경망에서 가장 최고의 인기를 얻고 있는 함수
        - 은닉층에서 ReLU함수들을 사용하는 것이 일반적
        - 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환
        - 0보다 크면 자기 자신 내보내고(linear), 0보다 작으면 0을 출력하는 함수

      - **소프트맥스 함수(Softmx function)**

        ![image-20210810181001758](https://user-images.githubusercontent.com/76864400/128843474-984c3178-d401-4a66-bd16-4a7f50a776b3.png)

        - 시그모이드 함수처럼 출력층의 뉴런에서 주로 사용
        - 시그모이드 함수가 이진 분류 문제(Binary Classification)에 사용된다면, 소프트맥스 함수는 다중 클래스 분류 문제(MultiClass Classification) 에 주로 사용됨

